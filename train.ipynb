# %% [markdown]
# # Piper Voice Training Notebook
# This notebook automates the process of training a Piper voice model, including:
# 1. Dataset preprocessing
# 2. Model training
# 3. Model exporting
# 4. Model testing

# %% [markdown]
# ## Step 1: Install Dependencies
# Ensure all required dependencies are installed.

# %%
!apt-get update
!apt-get install -y python3-dev espeak-ng
!pip install --upgrade pip wheel setuptools
!pip install pytorch-lightning jupyterlab

# Install Piper
!git clone https://github.com/rhasspy/piper.git
%cd piper/src/python
!pip install -e .
!bash build_monotonic_align.sh
%cd ../../..

# %% [markdown]
# ## Step 2: Prepare Dataset
# Place your dataset in the `dataset` directory with the following structure:
# ```
# dataset/
# ├── metadata.csv
# ├── wav/
# │   ├── 1.wav
# │   ├── 2.wav
# │   └── ...
# ```

# %%
import os

# Create dataset directory
dataset_dir = "dataset"
os.makedirs(dataset_dir, exist_ok=True)

# Example: Create metadata.csv and wav directory
metadata_path = os.path.join(dataset_dir, "metadata.csv")
wav_dir = os.path.join(dataset_dir, "wav")
os.makedirs(wav_dir, exist_ok=True)

# Example metadata.csv content (single speaker)
with open(metadata_path, "w") as f:
    f.write("1|This is a test sentence.\n")
    f.write("2|Another test sentence.\n")

# %% [markdown]
# ## Step 3: Preprocess Dataset
# Run the preprocessing script to generate `config.json` and `dataset.jsonl`.

# %%
!python3 -m piper_train.preprocess \
  --language en-us \
  --input-dir {dataset_dir} \
  --output-dir training_dir \
  --dataset-format ljspeech \
  --single-speaker \
  --sample-rate 22050

# %% [markdown]
# ## Step 4: Train the Model
# Train the Piper voice model using the preprocessed dataset.

# %%
# Download a pre-trained model checkpoint (e.g., lessac medium quality)
!wget https://example.com/path/to/lessac/epoch=2164-step=1355540.ckpt -O lessac.ckpt

# Train the model
!python3 -m piper_train \
    --dataset-dir training_dir \
    --accelerator 'gpu' \
    --devices 1 \
    --batch-size 32 \
    --validation-split 0.0 \
    --num-test-examples 0 \
    --max_epochs 10000 \
    --resume_from_checkpoint lessac.ckpt \
    --checkpoint-epochs 1 \
    --precision 32

# %% [markdown]
# ## Step 5: Export the Model
# Export the trained model to ONNX format.

# %%
# Find the latest checkpoint
import glob
checkpoints = glob.glob("training_dir/lightning_logs/version_0/checkpoints/*.ckpt")
latest_checkpoint = checkpoints[-1]

# Export to ONNX
!python3 -m piper_train.export_onnx \
    {latest_checkpoint} \
    model.onnx

# Copy config.json
!cp training_dir/config.json model.onnx.json

# %% [markdown]
# ## Step 6: Test the Model
# Test the exported model by generating audio from text.

# %%
# Create a test sentence
test_sentence = "This is a test sentence generated by Piper."

# Generate audio
!echo '{test_sentence}' | \
  piper -m model.onnx --output_file test.wav

# Play the audio (requires IPython and sound playback support)
from IPython.display import Audio
Audio("test.wav")

# %% [markdown]
# ## Step 7: Monitor Training with TensorBoard
# Monitor training progress using TensorBoard.

# %%
# Start TensorBoard
%load_ext tensorboard
%tensorboard --logdir training_dir/lightning_logs

# %% [markdown]
# ## Step 8: Save the Model
# Save the trained model and related files for future use.

# %%
import shutil

# Create a directory for the final model
os.makedirs("final_model", exist_ok=True)

# Copy the ONNX model and config
shutil.copy("model.onnx", "final_model/model.onnx")
shutil.copy("model.onnx.json", "final_model/config.json")

# Compress the final model
shutil.make_archive("final_model", "zip", "final_model")

# %% [markdown]
# ## Done!
# Your trained Piper voice model is ready to use. You can download the `final_model.zip` file for deployment.