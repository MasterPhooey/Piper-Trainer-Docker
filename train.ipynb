{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piper Voice Training Notebook\n",
    "This notebook automates the process of training a Piper voice model, including:\n",
    "1. Dataset preprocessing\n",
    "2. Model training\n",
    "3. Model exporting\n",
    "4. Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Installation\n",
    "Ensure that Piper and its dependencies are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Piper installation\n",
    "import piper_train\n",
    "\n",
    "# Print Piper version\n",
    "print(f\"Piper version: {piper_train.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify GPU Availability\n",
    "Check if a GPU is available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Training will use the CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert and Replace Audio Files\n",
    "Convert all audio files in the `dataset/wav` folder to WAV format and replace the originals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory setup\n",
    "wav_dir = \"dataset/wav\"\n",
    "\n",
    "# Collect all audio files in the wav directory\n",
    "audio_files = list(Path(wav_dir).glob(\"*.*\"))  # This will match any file extension\n",
    "print(f\"Number of audio files found: {len(audio_files)}\")\n",
    "\n",
    "if audio_files:\n",
    "    corrupted_files = []\n",
    "\n",
    "    print(\"Converting and replacing audio files in WAV format...\")\n",
    "    for file_path in tqdm(audio_files, desc=\"Processing audio files\"):\n",
    "        try:\n",
    "            # Skip files that are already in WAV format\n",
    "            if file_path.suffix.lower() == \".wav\":\n",
    "                continue\n",
    "\n",
    "            # Attempt to load the file and handle any errors\n",
    "            audio, sampling_rate = sf.read(file_path)\n",
    "            \n",
    "            if audio is None or len(audio) == 0:\n",
    "                raise ValueError(f\"Empty or invalid audio data in file: {file_path}\")\n",
    "\n",
    "            # Resample audio to 16kHz (optional, adjust as needed)\n",
    "            target_sampling_rate = 16000\n",
    "            if sampling_rate != target_sampling_rate:\n",
    "                audio = scipy.signal.resample(audio, int(len(audio) * target_sampling_rate / sampling_rate))\n",
    "\n",
    "            # Save the audio as a WAV file, replacing the original file\n",
    "            output_path = file_path.with_suffix(\".wav\")  # Replace the extension with .wav\n",
    "            scipy.io.wavfile.write(\n",
    "                output_path,\n",
    "                target_sampling_rate,\n",
    "                (audio * 32767).astype(np.int16),  # Scale to 16-bit PCM\n",
    "            )\n",
    "\n",
    "            # Remove the original file if it's not already a WAV file\n",
    "            if file_path.suffix.lower() != \".wav\":\n",
    "                os.remove(file_path)\n",
    "\n",
    "        except (sf.LibsndfileError, ValueError, Exception) as e:\n",
    "            # Log the error and skip the file\n",
    "            print(f\"Error converting {file_path}: {e}\")\n",
    "            corrupted_files.append(str(file_path))\n",
    "\n",
    "    # Log corrupted files\n",
    "    if corrupted_files:\n",
    "        log_path = Path(wav_dir) / \"corrupted_files.log\"\n",
    "        with open(log_path, \"w\") as log_file:\n",
    "            log_file.writelines(f\"{file}\\n\" for file in corrupted_files)\n",
    "        print(f\"Logged corrupted files to {log_path}\")\n",
    "else:\n",
    "    print(\"No audio files found in the directory.\")\n",
    "\n",
    "print(\"Audio conversion and replacement complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Transcribe Audio Files\n",
    "Generate `metadata.csv` by transcribing all `.wav` files in the `wav/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "import csv\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")  # You can use \"small\", \"medium\", or \"large\" for better accuracy\n",
    "\n",
    "# Define paths\n",
    "wav_dir = \"dataset/wav\"\n",
    "metadata_path = \"dataset/metadata.csv\"\n",
    "\n",
    "# Create metadata.csv\n",
    "with open(metadata_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"|\")\n",
    "    \n",
    "    # Iterate over all .wav files in the wav directory\n",
    "    for filename in os.listdir(wav_dir):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            # Get the full path to the audio file\n",
    "            audio_path = os.path.join(wav_dir, filename)\n",
    "            \n",
    "            # Transcribe the audio file\n",
    "            result = model.transcribe(audio_path)\n",
    "            text = result[\"text\"].strip()\n",
    "            \n",
    "            # Write the ID and transcription to metadata.csv\n",
    "            id = filename[:-4]  # Remove the .wav extension\n",
    "            writer.writerow([id, text])\n",
    "            \n",
    "            print(f\"Transcribed {filename}: {text}\")\n",
    "\n",
    "print(f\"\\nMetadata file generated at: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify `metadata.csv`\n",
    "Check the generated `metadata.csv` file to ensure the transcriptions are accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the metadata.csv file\n",
    "metadata = pd.read_csv(metadata_path, delimiter=\"|\", header=None, names=[\"id\", \"text\"])\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Preprocess Dataset\n",
    "Run the preprocessing script to generate `config.json` and `dataset.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m piper_train.preprocess \\\n",
    "  --language en-us \\\n",
    "  --input-dir dataset \\\n",
    "  --output-dir training_dir \\\n",
    "  --dataset-format ljspeech \\\n",
    "  --single-speaker \\\n",
    "  --sample-rate 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model\n",
    "Train the Piper voice model using the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a pre-trained model checkpoint (e.g., lessac medium quality)\n",
    "!wget https://example.com/path/to/lessac/epoch=2164-step=1355540.ckpt -O lessac.ckpt\n",
    "\n",
    "# Train the model\n",
    "!python3 -m piper_train \\\n",
    "    --dataset-dir training_dir \\\n",
    "    --accelerator 'gpu' \\\n",
    "    --devices 1 \\\n",
    "    --batch-size 32 \\\n",
    "    --validation-split 0.0 \\\n",
    "    --num-test-examples 0 \\\n",
    "    --max_epochs 10000 \\\n",
    "    --resume_from_checkpoint lessac.ckpt \\\n",
    "    --checkpoint-epochs 1 \\\n",
    "    --precision 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Export the Model\n",
    "Export the trained model to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest checkpoint\n",
    "import glob\n",
    "checkpoints = glob.glob(\"training_dir/lightning_logs/version_0/checkpoints/*.ckpt\")\n",
    "latest_checkpoint = checkpoints[-1]\n",
    "\n",
    "# Export to ONNX\n",
    "!python3 -m piper_train.export_onnx \\\n",
    "    {latest_checkpoint} \\\n",
    "    model.onnx\n",
    "\n",
    "# Copy config.json\n",
    "!cp training_dir/config.json model.onnx.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the Model\n",
    "Test the exported model by generating audio from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test sentence\n",
    "test_sentence = \"This is a test sentence generated by Piper.\"\n",
    "\n",
    "# Generate audio\n",
    "!echo '{test_sentence}' | \\\n",
    "  piper -m model.onnx --output_file test.wav\n",
    "\n",
    "# Play the audio (requires IPython and sound playback support)\n",
    "from IPython.display import Audio\n",
    "Audio(\"test.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Monitor Training with TensorBoard\n",
    "Monitor training progress using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir training_dir/lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Export and Download the Model\n",
    "Export the trained model and provide download links for the ONNX model and JSON config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Define the source paths for the ONNX model and config\n",
    "onnx_source_path = \"model.onnx\"\n",
    "config_source_path = \"model.onnx.json\"\n",
    "\n",
    "# Define the destination paths for download\n",
    "onnx_destination_path = \"./piper_model.onnx\"\n",
    "config_destination_path = \"./piper_model.json\"\n",
    "\n",
    "# Copy the ONNX model and config to the current working directory\n",
    "shutil.copy(onnx_source_path, onnx_destination_path)\n",
    "shutil.copy(config_source_path, config_destination_path)\n",
    "\n",
    "# Define the JSON file content for Piper\n",
    "piper_json_data = {\n",
    "    \"dataset\": \"norman\",  # Update this with your dataset name\n",
    "    \"audio\": {\n",
    "        \"sample_rate\": 22050,  # Update this based on your model's sample rate\n",
    "        \"quality\": \"medium\"    # Update this based on your model's quality\n",
    "    },\n",
    "    \"espeak\": {\n",
    "        \"voice\": \"en\"  # Update this based on your model's espeak voice\n",
    "    },\n",
    "    \"language\": {\n",
    "        \"code\": \"en_US\",        # Update this based on your model's language\n",
    "        \"family\": \"en\",         # Language family\n",
    "        \"region\": \"US\",         # Region\n",
    "        \"name_native\": \"English\",  # Native language name\n",
    "        \"name_english\": \"English\", # English language name\n",
    "        \"country_english\": \"United States\"  # Country name in English\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"noise_scale\": 0.667,  # Noise scale for inference\n",
    "        \"length_scale\": 1.0,   # Length scale for inference\n",
    "        \"noise_w\": 0.8         # Noise width for inference\n",
    "    },\n",
    "    \"phoneme_type\": \"espeak\",  # Phoneme type (e.g., espeak or text)\n",
    "    \"phoneme_map\": {},         # Phoneme map (if applicable)\n",
    "    \"phoneme_id_map\": {\n",
    "        \" \": [3],  # Word separator\n",
    "        \"!\": [4],  # Exclamation mark\n",
    "        \"$\": [2],  # End of utterance\n",
    "        \"'\": [5],  # Single quote\n",
    "        \"(\": [6],  # Open parenthesis\n",
    "        \")\": [7],  # Close parenthesis\n",
    "        \",\": [8],  # Comma\n",
    "        \"-\": [9],  # Hyphen\n",
    "        \".\": [10],  # Period\n",
    "        \"0\": [130],  # Number 0\n",
    "        \"1\": [131],  # Number 1\n",
    "        \"2\": [132],  # Number 2\n",
    "        \"3\": [133],  # Number 3\n",
    "        \"4\": [134],  # Number 4\n",
    "        \"5\": [135],  # Number 5\n",
    "        \"6\": [136],  # Number 6\n",
    "        \"7\": [137],  # Number 7\n",
    "        \"8\": [138],  # Number 8\n",
    "        \"9\": [139],  # Number 9\n",
    "        \":\": [11],  # Colon\n",
    "        \";\": [12],  # Semicolon\n",
    "        \"?\": [13],  # Question mark\n",
    "        \"^\": [1],  # Beginning of utterance\n",
    "        \"_\": [0],  # Padding\n",
    "        \"a\": [14],  # Phoneme a\n",
    "        \"b\": [15],  # Phoneme b\n",
    "        \"c\": [16],  # Phoneme c\n",
    "        \"d\": [17],  # Phoneme d\n",
    "        \"e\": [18],  # Phoneme e\n",
    "        \"f\": [19],  # Phoneme f\n",
    "        \"g\": [20],  # Phoneme g\n",
    "        \"h\": [21],  # Phoneme h\n",
    "        \"i\": [22],  # Phoneme i\n",
    "        \"j\": [23],  # Phoneme j\n",
    "        \"k\": [24],  # Phoneme k\n",
    "        \"l\": [25],  # Phoneme l\n",
    "        \"m\": [26],  # Phoneme m\n",
    "        \"n\": [27],  # Phoneme n\n",
    "        \"o\": [28],  # Phoneme o\n",
    "        \"p\": [29],  # Phoneme p\n",
    "        \"q\": [30],  # Phoneme q\n",
    "        \"r\": [31],  # Phoneme r\n",
    "        \"s\": [32],  # Phoneme s\n",
    "        \"t\": [33],  # Phoneme t\n",
    "        \"u\": [34],  # Phoneme u\n",
    "        \"v\": [35],  # Phoneme v\n",
    "        \"w\": [36],  # Phoneme w\n",
    "        \"x\": [37],  # Phoneme x\n",
    "        \"y\": [38],  # Phoneme y\n",
    "        \"z\": [39]   # Phoneme z\n",
    "    },\n",
    "    \"num_symbols\": 256,  # Number of phonemes in the model\n",
    "    \"num_speakers\": 1,   # Number of speakers in the model\n",
    "    \"speaker_id_map\": {},  # Speaker ID map (if applicable)\n",
    "    \"piper_version\": \"1.0.0\"  # Piper version\n",
    "}\n",
    "\n",
    "# Write the JSON file\n",
    "with open(config_destination_path, \"w\") as json_file:\n",
    "    json.dump(piper_json_data, json_file, indent=2)\n",
    "\n",
    "# Generate download links for both files\n",
    "print(\"Download your files:\")\n",
    "print(\"ONNX Model:\")\n",
    "display(FileLink(onnx_destination_path))\n",
    "print(\"\\nJSON Config:\")\n",
    "display(FileLink(config_destination_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
